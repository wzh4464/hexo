---
title: 共现概率
---
# 共现概率

这个式子是约束信息理论共聚类（CITCC）中用来逼近 $p(d_m, v_i)$ 的函数 $q(d_m, v_i)$ 的定义。其中，$\hat{d}_{k_d}$ 和 $\hat{v}_{k_v}$ 是文档和单词的聚类标签（即属于哪个聚类），$k_d$ 和 $k_v$ 分别表示文档和单词的聚类索引。$p\left(\hat{d}_{k_d}, \hat{v}_{k_v}\right)$ 表示文档和单词的聚类的联合概率。$p(d_m \mid \hat{d}_{k_d})$ 表示给定文档聚类标签 $\hat{d}_{k_d}$ 的条件下，文档 $d_m$ 出现的条件概率。$p(v_i \mid \hat{v}_{k_v})$ 表示给定单词聚类标签 $\hat{v}_{k_v}$ 的条件下，单词 $v_i$ 出现的条件概率。该式子的含义是：通过联合文档和单词的聚类标签来计算文档 $d_m$ 和单词 $v_i$ 的共现概率，从而逼近真实的概率分布 $p(d_m, v_i)$。

Problem Formulation
Denote the document set and word set as $\mathcal{D}=$ $\left\{d_1, d_2, \ldots, d_M\right\}$ and $\mathcal{V}=\left\{v_1, v_2, \ldots, v_V\right\}$. Then the joint probability of $p\left(d_m, v_i\right)$ can be computed based on the cooccurrence count of $d_m$ and $v_i$. For hard clustering problems, shown by Dhillon, Mallela, and Modha (2003), a function
$$
q\left(d_m, v_i\right)=p\left(\hat{d}_{k_d}, \hat{v}_{k_v}\right) p\left(d_m \mid \hat{d}_{k_d}\right) p\left(v_i \mid \hat{v}_{k_v}\right)
$$
where $\hat{d}_{k_d}$ and $\hat{v}_{k_v}$ are cluster indicators, $k_d$ and $k_v$ are the cluster indices, is used to approximate $p\left(d_m, v_i\right)$ by minimizing the Kullback-Leibler (KL) divergence:
$$
\begin{aligned}
& D_{K L}(p(\mathcal{D}, \mathcal{V}) \| q(\mathcal{D}, \mathcal{V})) \\
= & D_{K L}(p(\mathcal{D}, \mathcal{V}, \hat{\mathcal{D}}, \hat{\mathcal{V}}) \| q(\mathcal{D}, \mathcal{V}, \hat{\mathcal{D}}, \hat{\mathcal{V}})) \\
= & \sum_{k_d}^{K_d} \sum_{d_m: l_{d_m}=k_d} p\left(d_m\right) D_{K L}\left(p\left(\mathcal{V} \mid d_m\right) \| p\left(\mathcal{V} \mid \hat{d}_{k_d}\right)\right) \\
= & \sum_{k_v}^{K_v} \sum_{v_i: l_{v_i}=k_v} p\left(v_i\right) D_{K L}\left(p\left(\mathcal{D} \mid v_i\right) \| p\left(\mathcal{D} \mid \hat{v}_{k_v}\right)\right)
\end{aligned}
$$
where $\hat{\mathcal{D}}$ and $\hat{\mathcal{V}}$ are the cluster sets, $p\left(\mathcal{V} \mid \hat{d}_{k_d}\right)$ denotes a multinomial distribution based on the probabilities $\left.\left(p\left(v_1 \mid \hat{d}_{k_d}\right), \ldots, p\left(v_V \mid \hat{d}_{k_d}\right)\right)^T, \quad p\left(v_i \mid \hat{d}_{k_d}\right)\right)=$ $p\left(v_i \mid \hat{v}_{k_v}\right) p\left(\hat{v}_{k_v} \mid \hat{d}_{k_d}\right)$ and $p\left(v_i \mid \hat{v}_{k_v}\right)=p\left(v_i\right) / p\left(l_{v_i}=\hat{v}_{k_v}\right)$ due to hard clustering labels. Symmetrically we can define the probability for words: $p\left(\mathcal{D} \mid \hat{v}_{k_v}\right)$ denotes a multinomial distribution based on the probabilities $\left.\left(p\left(d_1 \mid \hat{v}_{k_v}\right), \ldots, p\left(d_V \mid \hat{v}_{k_v}\right)\right)^T, \quad p\left(d_i \mid \hat{v}_{k_v}\right)\right)=$ $p\left(d_i \mid \hat{d}_{k_d}\right) p\left(\hat{d}_{k_d} \mid \hat{v}_{k_v}\right)$ and $p\left(d_i \mid \hat{d}_{k_d}\right)=p\left(d_i\right) / p\left(l_{d_i}=\hat{d}_{k_d}\right)$.

这里 $\hat{d}_{k_d}$ 和 $\hat{v}_{k_v}$ 都是指示器变量，用于表示文档和词汇所属的簇。具体来说，$\hat{d}_{k_d}$ 表示第 $k_d$ 个簇，$\hat{v}_{k_v}$ 表示第 $k_v$ 个簇。因此，$\hat{d}_{k_d}$ 和 $\hat{v}_{k_v}$ 的取值为 ${0,1}$，其中 $\hat{d}_{k_d} = 1$ 表示文档 $d_m$ 属于第 $k_d$ 个簇，$\hat{v}_{k_v} = 1$ 表示词汇 $v_i$ 属于第 $k_v$ 个簇。而 $p(\hat{d}_{k_d}, \hat{v}_{k_v})$ 则表示簇 $\hat{d}_{k_d}$ 和 $\hat{v}_{k_v}$ 同时出现的概率。

假设我们有两个文档和三个词汇，它们的共现情况如下表所示：

|  | Word 1 | Word 2 | Word 3 |
| --- | --- | --- | --- |
| Doc 1 | 2 | 3 | 1 |
| Doc 2 | 0 | 1 | 3 |

现在我们要将这些文档和词汇聚类成两个簇。我们可以将 $\hat{d}_{k_d}$ 和 $\hat{v}_{k_v}$ 初始化为随机值。例如，我们可以假设第一个文档属于第一个簇，第二个文档属于第二个簇，第一个词汇属于第一个簇，第二个词汇和第三个词汇属于第二个簇，即：

$$
\hat{d}_{1}=1, \hat{d}_{2}=0, \hat{v}_{1}=1, \hat{v}_{2}=0, \hat{v}_{3}=0 $$

然后，我们可以使用公式 $q(d_m, v_i) = p(\hat{d}_{k_d}, \hat{v}_{k_v}) p(d_m \mid \hat{d}_{k_d}) p(v_i \mid \hat{v}_{k_v})$ 来计算每个文档和词汇属于每个簇的概率。例如，对于第一个文档和第一个词汇，它们属于第一个簇的概率为：
 
$$\begin{aligned} q(d_1, v_1) &= p(\hat{d}_{k_d}, \hat{v}_{k_v}) p(d_m \mid \hat{d}_{k_d}) p(v_i \mid \hat{v}_{k_v}) \\ &= p(\hat{d}_{1}, \hat{v}_{1}) p(d_1 \mid \hat{d}_{1}) p(v_1 \mid \hat{v}_{1}) \\ &= p(\hat{d}_{1}=1, \hat{v}_{1}=1) p(d_1 \mid \hat{d}_{1}=1) p(v_1 \mid \hat{v}_{1}=1) \\ &= ? \times ? \times ? \end{aligned}$$
其中，$p(\hat{d}_{1}, \hat{v}_{1})$ 表示第一个文档和第一个词汇同时属于第一个簇的概率，$p(d_1 \mid \hat{d}_{1})$ 表示第一个文档属于第一个簇的条件概率，$p(v_1 \mid \hat{v}_{1})$ 表示第一个词汇属于第一个簇的条件概率。这些概率需要根据先前的簇标签和统计信息来计算。

这段文字描述了一种基于硬聚类的文档和词汇聚类方法。文档集合和词汇集合分别用$\mathcal{D}$和$\mathcal{V}$表示。根据文档$d_m$和词汇$v_i$的共现次数计算它们的联合概率$p\left(d_m, v_i\right)$。为了解决硬聚类问题，Dhillon, Mallela和Modha（2003）提出了一个函数$q\left(d_m, v_i\right)$来近似$p\left(d_m, v_i\right)$，并通过最小化Kullback-Leibler（KL）散度实现。

在这个函数中，$\hat{d}_{k_d}$和$\hat{v}_{k_v}$是聚类指示器，而$k_d$和$k_v$是聚类索引。$\hat{\mathcal{D}}$和$\hat{\mathcal{V}}$表示聚类集合。$p\left(\mathcal{V} \mid \hat{d}_{k_d}\right)$表示一个基于概率的多项分布，这些概率由硬聚类标签得出。对于词汇，我们可以对称地定义概率：$p\left(\mathcal{D} \mid \hat{v}_{k_v}\right)$表示一个基于概率的多项分布。